## Executive summary
The Autoreduction **live** service was not affected. This did not cause any missed runs or errors in processing. This was purely a monitoring failure.

The `elasticsearch` node failed earlier than expected. The failure was mitigated by adding new storage, and moving the `elasticsearch` data onto it, then starting the monitoring again.

----

### Leadup

We’ve been aware that the Elasticsearch could run out of disk space, since about midway into the cycle. Alarms were in place to warn us of low disk space, which triggered when there was 5GB free. Remoting into the node and running `df -h` was reporting ~5.3GB free on `/`. 

From observing the disk usage over the cycle, we estimated that it is roughly 1GB/day, for the current infrastructure setup. 

When the alarm triggered on the 2021-06-15 15:57, it seemed that the node would manage the rest of the day, and the following day, which was the last day of cycle (16/06/2021). 

### Fault

The time from the alarm until failure was underestimated. The `elasticsearch` service went down about two hours after the alarm. 

Due to `elasticsearch` being run on a single node, rather than a cluster of nodes, the prevention action to relocate data from this node was impossible, as there was no other node. 

This led to the node being marked down as `read-only` when it went below 5% free space:

```
[2021-06-15T16:36:12,871][WARN ][o.e.c.r.a.DiskThresholdMonitor] [node-1] flood stage disk watermark [95%] exceeded on [h-xo30sfRhu0mCBf50i8kQ][node-1][/var/lib/elasticsearch/nodes/0] free: 3.9gb[4.9%], all indices on this node will be marked read-only
```

### Detection

The incident was detecting during check-in as part of the on-call support. Kibana failed to update the live graphs that showed the status of `ActiveMQ` queue processing, after which the `elasticsearch` logs were checked via less `/var/log/elasticsearch/autoreduce-es.log` (location is pre-set, and `autoreduce-es.log` comes from the name of the local cluster). This showed `TOO_MANY_REQUESTS/12/disk usage exceeded flood-stage watermark, index has read-only-allow-delete block` errors.

### Root causes

#### _Run a 5-whys analysis to understand the true causes of the incident_

Why did the elasticsearch node fail earlier than expected? It ran out of disk space.

Why did it run out of disk space? We did not extend the disk storage.

Why did we not extend the disk storage? Our estimation for “from alarm until failure” was wrong, and made us think it would fail later.

Why was our estimation for “from alarm until failure” wrong? It did not account for Elasticsearch making the node read-only when at 95% capacity.

### Mitigation and resolution

#### _What steps did you take to resolve this incident?_

A new volume was added via the `openstack` interface at `/dev/vdb`. This was formatted with `ext2` and mounted at `/mnt/external`. The `elasticsearch` data was copied from `/var/lib/elasticsearch` to `/mnt/external/elasticsearch`. The service was started again and has continued working.

The data had to be copied twice - the first time the elasticsearch service wasn’t shut down properly, so some indices were changed during copying, corrupting the result. The second time it was fully turned off, and copying worked.

### Lessons learnt

#### _What went well? What could have gone better? What else did you learn?_

This failure was expected, and there was a plan in place for when it failed: mount a new volume using the `openstack` interface, copy the data over, change the data location in the `elasticsearch` config, and start the service again. We did not have to come up with a brand new solution at the time of handling the incident.

There was still an unexpected step - the newly mounted volume has to be formatted before the OS can use it. Overall, there was no official documentation on the exact set of steps necessary to mitigate this, so if anyone else had been on-call, they likely would have taken much longer to fix this, or would have had to leave it until the next working day.

To prevent this from happening next cycle the following will be done:

- A second elasticsearch instance will be running, monitoring the main one. This will be monitoring a lot less, so it can be run on a thin node.

- Low disk usage alarm will be more aggressive, i.e. at 10% free disk space, and we will extend the volume first thing post the alarm

- The elasticsearch node itself will be made with a much larger volume. For this cycle it had 100GB. For next we will aim for it to have 500GB.

- Running the elasticsearch service on more than one node in a cluster setup. This would allow elasticsearch to balance disk usage and relocate data, if necessary, to save nodes from failing, and could have prevented this incident.

- Prepare proper documentation + commands on how to stop elasticsearch, mount volume, format it, copy elasticsearch data, change the configuration, and start the service again [AR-1456](https://autoreduce.atlassian.net/browse/AR-1456)

